## Overview

| Developed by | Guardrails AI |
| --- | --- |
| Date of development | Feb 15th, 2024 |
| Validator type | Safety |
| Blog | - |
| License | Apache 2 |
| Input/Output | Input, Output |

## Description

This validator monitors any text (input or output) and detects secrets present in the text. Under-the-hood, the validator uses the `detect-secrets` library to check whether the text contains any secrets. If any secrets are detected, the validator fails and returns the text with the secrets replaced with asterisks. Otherwise, the validator returns the generated code snippet.

### Resources required

- Dependencies: `detect-secrets`

## Installation

```bash
$ gudardrails hub install hub://guardrails/detect_secrets
```

## Usage Examples

### Validating string output via Python

In this example, we apply the validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import DetectSecrets
from guardrails import Guard

val = DetectSecrets(
    on_fail="fix"
)

guard = Guard.from_string(validators=[val])

guard.parse("Some message without secrets")  # Validator passes
guard.parse("Some message with sk-******")  # Validator fails
```

### Validating JSON output via Python

In this example, we apply the validator to a string field of a JSON output generated by an LLM.

```python
# Import Guard and Validator
from pydantic import BaseModel
from guardrails.hub import DetectSecrets
from guardrails import Guard

val = DetectSecrets(
    on_fail="fix"
)

# Create Pydantic BaseModel
class UserMsg(BaseModel):
    timestamp: int
    content: str = Field(
        description="Msg by user", validators=[val]
    )

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=UserMsg)

# Run LLM output generating JSON through guard
guard.parse("""
{
    "timestamp": 1707718924,
    "content": "Hi, can you help me with my account?"
}
""")
```

## API Reference

`__init__`
- on_fail: The policy to enact when a validator fails.
  
